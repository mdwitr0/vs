## Сканер и индексер пиратских сайтов + фронтенд

### 1.1 Детектор технологий сайтов

**Задачи:**
- Разработка Go-модуля для определения CMS и технологий
- Анализ HTML-структуры для выявления DLE, Wordpress, SPA
- Определение наличия sitemap.xml
- Детекция SSR vs CSR рендеринга
- Универсальный детектор капчи

**Детали реализации:**
- Создать базовые паттерны для распознавания CMS через meta-теги, специфичные классы, комментарии в HTML
- Проверка заголовков ответа на наличие характерных headers
- Анализ структуры URL для определения паттернов роутинга
- Простая эвристика для определения необходимости headless browser через проверку наличия большого количества JS и отсутствия контента в initial HTML
- Детекция капчи через поиск характерных скриптов и элементов

**Оценка:** 12-16 часов

**Риски:**
- Некоторые гибридные сайты могут неправильно определяться
- Паттерны CMS могут меняться со временем

### 1.2 Модульная система индексации

**Компоненты:**

#### 1.2.1 Универсальный сканер
- Парсинг sitemap.xml с обработкой различных форматов
- HTTP-парсинг страниц с извлечением ссылок
- Анализ URL-паттернов для определения ID контента
- Попытка адаптации slug в ID через регулярные выражения

**Оценка:** 20-24 часа

#### 1.2.2 Специфичные сканеры (опционально, не оцениваем)
- **DLE-сканер**: использование sitemap.xml и встроенного поиска `/?do=search&mode=advanced&subaction=search&story=<название>`
- **Wordpress-сканер**: работа с sitemap.xml, плагинами SEO и REST API
- **SPA-сканер**: интеграция headless browser через chromedp для рендеринга JS-контента

**Примечание:** Специфичные сканеры можно добавить позже при необходимости, на первом этапе достаточно универсального сканера для SSR-сайтов

### 1.3 Хранилище проиндексированных страниц

**Структура данных (MongoDB):**
```json
{
  "site_id": "string",
  "url": "string",
  "title": "string",
  "year": "int",
  "external_ids": {
    "kpid": "string",
    "imdb_id": "string",
    "mal_id": "string",
    "shikimori_id": "string",
    "mydramalist_id": "string"
  },
  "meta_tags": "object",
  "player_iframe_url": "string",
  "indexed_at": "timestamp",
  "last_checked_at": "timestamp"
}
```

**Индексы:**
- Составной индекс по site_id + external_ids для быстрого поиска совпадений
- Индекс по url для проверки дубликатов
- Индекс по indexed_at для работы шедулера

**Оценка:** 8-12 часов

### 1.4 Система отслеживания статуса сайтов

**Функционал:**
- Мониторинг HTTP-кодов при каждом сканировании
- Фиксация downtime и последнего успешного скана
- Автоматическое отключение сканирования для недоступных сайтов
- Периодическая повторная проверка недоступных сайтов

**Структура данных:**
```json
{
  "domain": "string",
  "status": "enum(active, down, blocked)",
  "last_success": "timestamp",
  "last_check": "timestamp",
  "failure_count": "int",
  "detected_tech": "string",
  "scan_config": {
    "scanner_type": "string",
    "requires_browser": "bool",
    "has_captcha": "bool"
  }
}
```

**Оценка:** 8 часов

### 1.5 Шедулер для автоматического сканирования

**Реализация через gocron:**
- Конфигурируемый интервал для каждого сайта
- Очередь задач для избежания перегрузки
- Приоритизация сайтов по активности
- Контроль конкурентности для предотвращения спама

**Логика:**
- Выборка сайтов готовых к сканированию по времени last_check
- Фильтрация по статусу (только active)
- Распределение нагрузки через rate limiting
- Логирование результатов

**Оценка:** 12 часов

### 1.6 API для ручного запуска и управления

**Endpoints:**
- `POST /api/sites/scan` - запуск сканирования списка сайтов
- `POST /api/sites` - добавление нового сайта с CSV-загрузкой
- `GET /api/sites` - список сайтов с фильтрацией и пагинацией
- `GET /api/scan-tasks/:id` - статус задачи сканирования
- `GET /api/reports/violations` - отчет по нарушениям

**Оценка:** 16-20 часов

### 1.7 Фронтенд: страница управления сайтами

**Функционал:**
- Таблица сайтов с колонками: домен, дата последней проверки, статус, количество нарушений
- Фильтрация по дате обхода
- Фильтрация по факту нарушения (есть/нет нарушения)
- Поиск по домену
- Загрузка CSV с доменами для массового добавления
- Кнопка ручного запуска сканирования выбранных сайтов
- Отображение прогресса текущего сканирования через polling

**Приоритет:** Основной инструмент для менеджеров - фокус на быстром доступе к данным о нарушениях

**Технологии:** React + shadcn/ui или аналогичная библиотека

**Оценка:** 5-10 часов

### 1.8 Фронтенд: страница управления контентом

**Функционал:**
- Таблица с контентом: название, год, внешние ID, количество нарушений, количество сайтов
- Форма добавления контента с полями: название, kpid, imdbID, malID/shikimoriID, mydramalistID
- Массовая загрузка контента через CSV
- Поиск и фильтрация
- Детальная страница контента с отчетом по нарушениям
- Отчет включает список сайтов с нарушениями и URL страниц
- Экспорт отчета в CSV

**Оценка:** 5-10 часов